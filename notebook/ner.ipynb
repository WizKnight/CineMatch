{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import ast\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading The Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "file_path = os.path.join(current_directory,\"data\", \"raw\", \"final_movie_data.csv\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting cast and genres columns to list\n",
    "df['cast'] = df['cast'].apply(lambda x: ast.literal_eval(x) if isinstance(x,str) else [])\n",
    "df['genres'] = df['genres'].apply(lambda x: ast.literal_eval(x) if isinstance(x,str) else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining text fields for NER\n",
    "df['text_for_ner'] = df['overview'] + ' ' + df['genres'].apply(lambda x: ' '.join(x)) + ' ' + df['cast'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MovieDataset(Dataset):\n",
    "#    def __init__(self, texts, tokenizer, max_length=128):\n",
    "#        self.texts = texts\n",
    "#        self.tokenizer = tokenizer\n",
    "#        self.max_length = max_length\n",
    "        \n",
    "        \n",
    "#    def __len__(self):\n",
    "#        return len(self.texts)\n",
    "    \n",
    "    \n",
    "#    def __getitem__(self, idx):\n",
    "#        text = self.texts[idx]\n",
    "        \n",
    "#        encoding = self.tokenizer(\n",
    "#            text,\n",
    "#            return_tensors=\"pt\",\n",
    "#            max_length=self.max_length,\n",
    "#            truncation=True,\n",
    "#            padding=\"max_length\"\n",
    "#        )\n",
    "#        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained Model\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NER Pipeine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    # Check if the text is valid (not NaN and not just whitespace)\n",
    "    if pd.notna(text) and not (isinstance(text, str) and text.isspace()): \n",
    "        try:\n",
    "            entities = ner_pipeline(text)\n",
    "            return {ent['entity_group']: ent['word'] for ent in entities}\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {e}\")\n",
    "            return {}  # Return an empty dictionary in case of an error\n",
    "    else:\n",
    "        return {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MISC': 'Animation Science Fiction Action', 'ORG': 'Justice League', 'PER': '##ke Amadi'}\n"
     ]
    }
   ],
   "source": [
    "example_text = df['text_for_ner'].iloc[7]\n",
    "print(extract_entities(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER completed and data saved!\n"
     ]
    }
   ],
   "source": [
    "df['extracted_entities'] = df['text_for_ner'].apply(extract_entities)\n",
    "\n",
    "directory = \"E:\\\\Git Uploads\\\\CineMatch\\\\notebook\\\\data\\\\processed\"\n",
    "df.to_csv(os.path.join(directory, \"movies_with_entities.csv\"), index=False)\n",
    "\n",
    "print(\"NER completed and data saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
